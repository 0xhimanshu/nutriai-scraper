# NutriAI Scraper

ğŸ¤– **Global Restaurant & Menu Data Collection System**

Comprehensive scraping solution for NutriAI app - collects restaurant data and menus from delivery platforms worldwide with CAPTCHA handling.

## ğŸš€ Features

- **Google Places API Integration** - Restaurant discovery across 15+ major cities
- **CAPTCHA Solving** - 2captcha service integration for protected sites
- **Stealth Scraping** - Puppeteer with anti-detection measures
- **Global Coverage** - India, UAE, USA, UK markets
- **Platform Support** - Zomato, Swiggy, Talabat, DoorDash, Uber Eats
- **MySQL Database** - Structured data storage with progress tracking
- **Rate Limiting** - Respectful scraping with delays

## ğŸŒ Coverage

### Cities Supported:
- **India:** Mumbai, Delhi, Bangalore, Chennai, Hyderabad, Pune
- **UAE:** Dubai, Abu Dhabi, Sharjah  
- **USA:** New York, Los Angeles, Chicago
- **UK:** London, Manchester

### Expected Results:
- **5,000-20,000 restaurants** across all cities
- **Comprehensive menu data** with prices, descriptions, ingredients
- **Restaurant details** including ratings, photos, hours, location

## ğŸ“Š Performance

- **API Calls:** 10,000-50,000 (monitor Google Places quota)
- **Runtime:** 2-6 hours for full scraping
- **CAPTCHA Cost:** $2-50/month depending on volume
- **Success Rate:** 85-95% with proper configuration

## âš¡ Quick Start

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/nutriai-scraper.git
cd nutriai-scraper

# Install dependencies
npm install

# Configure environment
cp .env.example .env
# Add your API keys to .env file

# Initialize database (run this from main NutriAI app)
curl http://localhost:3000/api/db/init

# Start scraping
npm run restaurants  # Scrape restaurant data
npm run menus       # Scrape menu data  
npm run all         # Run everything
```

## ğŸ”§ Configuration

### Required API Keys:
```bash
# .env file
GOOGLE_PLACES_API_KEY=your_google_places_api_key
TWOCAPTCHA_API_KEY=your_2captcha_api_key
DB_HOST=localhost
DB_USER=root
DB_PASSWORD=your_mysql_password
DB_NAME=nutriai_dev
```

### Optional Platform Keys:
```bash
ZOMATO_API_KEY=your_zomato_api_key
SWIGGY_PARTNER_KEY=your_swiggy_partner_key
```

## ğŸ›¡ï¸ CAPTCHA Handling

### Automatic CAPTCHA Solving:
- **Service:** 2captcha integration
- **Cost:** $1-3 per 1000 CAPTCHAs
- **Success Rate:** 90%+ for reCAPTCHA v2/v3
- **Fallback:** Manual intervention prompts

### Supported CAPTCHA Types:
- reCAPTCHA v2 (checkbox)
- reCAPTCHA v3 (invisible)
- hCaptcha
- Custom image CAPTCHAs

## ğŸ—ï¸ Architecture

```
nutriai-scraper/
â”œâ”€â”€ data-scraper.js          # Main scraping engine
â”œâ”€â”€ package.json             # Dependencies & scripts
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ .env.example            # Environment template
â”œâ”€â”€ captcha-solutions.md    # CAPTCHA handling guide
â””â”€â”€ .gitignore             # Git ignore rules
```

## ğŸ“‹ Usage Examples

### Restaurant Discovery:
```bash
node data-scraper.js restaurants
```

### Menu Scraping:
```bash
node data-scraper.js menus
```

### Full Pipeline:
```bash
node data-scraper.js all
```

## ğŸ“Š Database Schema

The scraper integrates with the main NutriAI database:

- **restaurants** - Restaurant information
- **menu_items** - Dish details with pricing
- **scraping_jobs** - Progress tracking
- **user_interactions** - Analytics data

## ğŸ”’ Legal & Ethical Guidelines

### Best Practices:
- âœ… Respect robots.txt files
- âœ… Use official APIs when available
- âœ… Rate limit all requests
- âœ… Follow platform terms of service
- âœ… Provide value to restaurants through referrals

### Partnership Approach:
- Contact platforms for official API access
- Offer revenue sharing for restaurant referrals
- Position as customer acquisition channel

## ğŸš¨ Troubleshooting

### Common Issues:
- **Database Connection:** Check MySQL credentials and server status
- **API Errors:** Verify Google Places API key and billing
- **CAPTCHA Failures:** Ensure 2captcha account has sufficient balance
- **Rate Limiting:** Increase delays between requests

### Monitoring:
```bash
# Check scraping progress
SELECT * FROM scraping_jobs ORDER BY created_at DESC LIMIT 5;

# View restaurant count by city
SELECT city, country, COUNT(*) as restaurants 
FROM restaurants 
GROUP BY city, country;

# Menu items statistics  
SELECT COUNT(*) as total_menu_items,
       AVG(price) as avg_price,
       COUNT(DISTINCT restaurant_id) as restaurants_with_menus
FROM menu_items;
```

## ğŸ“ˆ Scaling

### Performance Optimization:
- Run multiple scraper instances with different city filters
- Use proxy rotation for high-volume scraping
- Implement queue system for menu scraping jobs
- Add Redis caching for frequently accessed data

### Cost Management:
- Monitor CAPTCHA usage and optimize trigger conditions
- Implement smart retry logic to reduce failed attempts
- Use official APIs where possible to avoid scraping costs

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add your improvements
4. Test thoroughly
5. Submit a pull request

## ğŸ“ License

MIT License - see LICENSE file for details.

## ğŸ”„ Generated with Claude Code

This scraper was built using Claude Code for rapid development and deployment.

---

**âš ï¸ Important:** Always respect website terms of service and implement appropriate rate limiting. Consider reaching out to platforms for partnership opportunities before large-scale scraping.
